{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Be sure to install the latest Gym environments on your local machine. \n",
    "Navigate to .../rl-portfolio/Gym Environments/Portfolio_Management/\n",
    "and run the command:\n",
    "pip install -e .\n",
    "'''\n",
    "\n",
    "import numpy             as np\n",
    "import multiprocessing   as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import Portfolio_Gym\n",
    "import A2C_X as A2C\n",
    "import Wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Settings & descriptions for the AC Agent '''\n",
    "\n",
    "'''\n",
    "    Actor_Hypers & Critic_Hypers | dict\n",
    "        'Learning Rate' | float, list | The learning rate of the Actor.\n",
    "        'Epoch'         | int         | The number of times each observation passes through the network.\n",
    "        'Network Size'  | int         | The size of the Actor NN\n",
    "        'Activation'    | string      | The activation of the Actor. Acceptable inputs include ['Relu', 'Sigmoid', 'Tanh', 'Softplus']\n",
    "        'Batch Size'    | int         | The number of observations which pass through the network in each pass.\n",
    "        'Alpha'         | float       | L2 regularization coefficient for the Actor.\n",
    " \n",
    "     Gamma | float\n",
    "        The discount rate for reward recieved by the agent.\n",
    "\n",
    "    Sigma_Range | list\n",
    "        A list of two floats, the first gives the starting sigma, and the last giving the terminal sigma. Sigma here referes to the sigma of the policy.\n",
    "\n",
    "    Sigma_Anneal | float\n",
    "        The fraction of training episodes which must pass before sigma decays to its terminal value.\n",
    "\n",
    "    Retrain_Frequency | int\n",
    "        The number of episodes between refits of the Actor and Critic\n",
    "\n",
    "    Action_Space_Clip | float\n",
    "        The value at which to clip the levergae the agent can take, to prevent it from randomly acting too agressively.\n",
    "\n",
    "    Experiance_Mode | string\n",
    "        A key which indicates the method to be used to generate experiance targets. Acceptable inputs include: ['Monte_Carlo', 'TD_1', 'TD_Lambda']\n",
    "\n",
    "    TD_Lambda | float, list\n",
    "        The lambda to use if using Experiance_Mode 'TD_Lambda'. If a float is passes lambda is constant. If a list of length 3 is passed then Lambda will fall from the 0th value to the 1st value, and will take the 2nd value fraction of training episodes to do so. i.e. by default will fall from 1 to 0.8 across 0.5 of the training episodes. A value of 1 is equivalent to Monte_Carlo, and a value of zero is equivalent to TD_1.\n",
    "\n",
    "    Monte_Carlo_Frac | float\n",
    "        The fraction of episodes to run overwriting Experiance_Mode with 'Monte_Carlo', as this method is most stable when the critic is poorly trained at the start of the training sequence.\n",
    "\n",
    "    Ignore_Actor_Frac | float\n",
    "        The fraction of episodes to train only Critic. Prevents the Actor being trained on nonsense at the start of training.\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "Reading the plots:\n",
    "In order from left to right the plots represent:\n",
    "    1. The critic prediction vs wealth, taken at 5 snapshots equally spaced throughout training. \n",
    "       The black curve represents the true utility function which the critic should approximate.\n",
    "       \n",
    "    2. Action vs wealth, taken at 5 snapshots equally spaced throughout training. Ideally this should be a horizontal line, \n",
    "       as wealth should not impact leverage, however this plot generated with the factor at its stationary point, and the \n",
    "       relation could change dramatically as the factor value fluctuates.\n",
    "       \n",
    "    3. Actor vs Factor, taken at 5 snapshots equally spaced throughout training. Ideally should see a linear relation with \n",
    "       positive gradient. \n",
    "       \n",
    "    4. The Mu of the policy vs training episode. (Note this is not the action taken, but instead the action the agent thinks is optimal)\n",
    "       A widening spread in this plot indicates that the agent is adjusting its action based upon variations in the state parameters, which\n",
    "       is the desired behaviour. A thin line indicates the action is independent of state parameters.\n",
    "       \n",
    "    5. Agent terminal utility vs merton terminal utility. Recalculated every 10,000 steps. Represents the agents performance were exploration \n",
    "       rate set to zero.\n",
    "       \n",
    "    6. Fraction of actions which are within 10% of the merton action. \n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Model Parameters.\n",
    "N_Eps  = int(5e5)\n",
    "\n",
    "Actor_Hypers   = {\"Learning Rate\" : 0.005,\n",
    "                  \"Epoch\"         : 1,\n",
    "                  \"Network Size\"  : [32,16,8],\n",
    "                  \"Activation\"    : \"Sigmoid\",\n",
    "                  \"Batch Size\"    : 60,\n",
    "                  \"Alpha\"         : 0.1}\n",
    "\n",
    "Critic_Hypers = {\"Learning Rate\" : 0.005,\n",
    "                 \"Epoch\"         : 10,\n",
    "                 \"Network Size\"  : [8,4],\n",
    "                 \"Activation\"    : \"Sigmoid\",\n",
    "                 \"Batch Size\"    : 60,\n",
    "                 \"Alpha\"         : 0.1}\n",
    "\n",
    "Gamma             = 0.999\n",
    "Sigma_Range       = [2, 0.5]\n",
    "Sigma_Anneal      = 1\n",
    "Retrain_Frequency = 20\n",
    "Action_Space_Clip = 75\n",
    "Experiance_Mode   = 'TD_Lambda'\n",
    "TD_Lambda         = 0.50\n",
    "Monte_Carlo_Frac  = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Simulated GBM Environment (With no Factors)\n",
    "'''\n",
    "\n",
    "# Function to facilitate mulitprocessing.\n",
    "def Run_0(seed):\n",
    "    np.random.seed(seed)\n",
    "    Env = gym.make('Simulated-v0')\n",
    "    Env.Set_Params(Max_Leverage = 100, Min_Leverage = -100)\n",
    "        \n",
    "    myAC = A2C.Actor_Critic(Env, Actor_Hypers, Critic_Hypers, Gamma = Gamma, Sigma_Range = Sigma_Range, Sigma_Anneal = Sigma_Anneal, Retrain_Frequency = Retrain_Frequency, \n",
    "                            Action_Space_Clip = Action_Space_Clip, Experiance_Mode = Experiance_Mode, TD_Lambda = TD_Lambda, Monte_Carlo_Frac = Monte_Carlo_Frac)\n",
    "    \n",
    "    myWrapper = Wrapper.Wrapper(myAC)\n",
    "    myWrapper.Train(N_Eps, Plot = ['Mu', 'Merton_Benchmark', 'Percent_Merton_Action'], Validate = True)\n",
    "    return None\n",
    "\n",
    "\n",
    "# Run the investigation...\n",
    "# with mp.Pool(mp.cpu_count()) as pool:\n",
    "#     _ = pool.map(Run_0, np.random.randint(0, int(1e9), 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "''' \n",
    "Simulated VAR Environment, Factor per 'Portfolio Choice Problems' by Brandt. (R2 about 0.033)     \n",
    "'''\n",
    "\n",
    "# Function to facilitate mulitprocessing.\n",
    "def Run_1(seed):\n",
    "    np.random.seed(seed)\n",
    "    Env = gym.make('Simulated-v1')\n",
    "    Env.Set_Params(Max_Leverage = 100, Min_Leverage = -100)\n",
    "        \n",
    "    myAC = A2C.Actor_Critic(Env, Actor_Hypers, Critic_Hypers, Gamma = Gamma, Sigma_Range = Sigma_Range, Sigma_Anneal = Sigma_Anneal, Retrain_Frequency = Retrain_Frequency, \n",
    "                            Action_Space_Clip = Action_Space_Clip, Experiance_Mode = Experiance_Mode, TD_Lambda = TD_Lambda, Monte_Carlo_Frac = Monte_Carlo_Frac)\n",
    "        \n",
    "    myWrapper = Wrapper.Wrapper(myAC)\n",
    "    myWrapper.Train(N_Eps, Plot = ['Mu', 'Merton_Benchmark', 'VAR_Benchmark'], Validate = True)\n",
    "    return None\n",
    "\n",
    "\n",
    "# Run the investigation...\n",
    "# with mp.Pool(mp.cpu_count()) as pool:\n",
    "#     _ = pool.map(Run_1, np.random.randint(0, int(1e9), 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n",
      " 53%|█████▎    | 265297/500000 [49:58<44:30, 87.88it/s]   "
     ]
    }
   ],
   "source": [
    "''' \n",
    "Historical Environment, Using Fama Market Average.   \n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "# 'ltr', 'corpr', 'CRSP_SPvw', 'CRSP_SPvwx', 'Mom', 'HML', 'SMB'\n",
    "def Run_Hist(seed):\n",
    "    np.random.seed(seed)\n",
    "    Env = gym.make('Historical-v0')\n",
    "    Env.Set_Params(Max_Leverage = 100, Min_Leverage = -100, State_Parameters = ['Mom', 'HML', 'SMB'])\n",
    "        \n",
    "    myAC = A2C.Actor_Critic(Env, Actor_Hypers, Critic_Hypers, Gamma = 0.999, Sigma_Range = [2, 0.5], Sigma_Anneal = 1, \n",
    "                            Retrain_Frequency = 20, Action_Space_Clip = 75, Experiance_Mode = 'TD_Lambda', TD_Lambda = 0.50, Monte_Carlo_Frac = 0.2)\n",
    "    \n",
    "    myWrapper = Wrapper.Wrapper(myAC)\n",
    "    myWrapper.Train(N_Eps, Plot = ['Mu', 'Merton_Benchmark'], Validate = True, Equity_Curve = True)\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Run the investigation...\n",
    "with mp.Pool(mp.cpu_count()) as pool:\n",
    "    _ = pool.map(Run_Hist, np.random.randint(0, int(1e9), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
